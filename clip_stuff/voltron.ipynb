{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNqhWfwYgsug4nsQBZ1YOpM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!git clone https://github.com/siddk/voltron-robotics\n","%cd voltron-robotics\n","!pip install -e .\n"],"metadata":{"id":"iYuTNAZLKC_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install voltron-robotics"],"metadata":{"id":"8MUOzCmo-JaS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import voltron\n","\n","voltron.available_models() #list of available models to choose from, ones specified in the paper are 'v-cond','v-dual','v-gen'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61JfJsev-P4P","executionInfo":{"status":"ok","timestamp":1715470289610,"user_tz":420,"elapsed":5115,"user":{"displayName":"Roy Y","userId":"06449485779154380910"}},"outputId":"3794642d-d0b7-449c-dba5-56212ec5782f"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['v-cond',\n"," 'v-dual',\n"," 'v-gen',\n"," 'v-cond-base',\n"," 'r-mvp',\n"," 'r-r3m-vit',\n"," 'r-r3m-rn50']"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["Below is code for V-cond."],"metadata":{"id":"jYVkx1ZzOzvd"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torchvision.io import read_image\n","import voltron\n","\n","# code for vcond, for single frame images (and optional language context)\n","# images tested from Yihe and behavior seem to have H,W or 784. preprocessed to H,W = 224, but when reshaping, can set to 784 as well and it still works\n","\n","def get_embeddings(path, vcond, preprocess, vector_extractor):\n","  image_tensor = read_image(path)\n","  if image_tensor.shape[0] != 3:  # Check if there's an alpha channel, 4 for behavior imgs\n","      image_tensor = image_tensor[:3, :, :]  # Keep only the first three channels (RGB)\n","\n","  img = preprocess(image_tensor)[None, ...].to(\"cuda\")\n","  lang = [\"\"] #Empty, can specify as needed for model\n","\n","  # Extract both multimodal AND vision-only embeddings!\n","  multimodal_embeddings = vcond(img, lang, mode=\"multimodal\")\n","  visual_embeddings = vcond(img, mode=\"visual\")\n","\n","  # Use the `vector_extractor` to output dense vector representations for downstream applications!\n","  #   => Pass this representation to model of your choice (object detector, control policy, etc.)\n","  representation = vector_extractor(multimodal_embeddings) #(1, 384) shape tensor\n","\n","  return multimodal_embeddings, visual_embeddings, representation\n","\n","\n","def reshape_vcond(path, vcond, preprocess, vector_extractor):\n","  multimodal,visual,vec_rep = get_embeddings(path, vcond, preprocess, vector_extractor)\n","  multimodal = multimodal.unsqueeze(-1) # Add extra dim for interpolate\n","  visual = visual.unsqueeze(-1)\n","  vec_rep = vec_rep.unsqueeze(-1)\n","  H,W = 224,224 # Behavior images were shape [4,784,784] when I was testing, but preprocess makes them 224x224. changing H,W to 224 or 784 both work\n","\n","  multimodal_resized = F.interpolate(multimodal, size=(H,W), mode='bilinear', align_corners=False)\n","  visual_resized = F.interpolate(visual, size=(H,W), mode='bilinear', align_corners=False)\n","  vec_rep = vec_rep.unsqueeze(-1)\n","  vec_rep_resized = F.interpolate(vec_rep, size=(H,W), mode='bilinear', align_corners=False)\n","\n","  multimodel_og_dim = multimodal_resized[0, 0, :, :]\n","  visual_og_dim = visual_resized[0, 0, :, :]\n","  vec_rep_og_dim = vec_rep_resized[0, 0, :, :]\n","\n","  return multimodel_og_dim, visual_og_dim, vec_rep_og_dim\n","\n","path = \"\" # TODO: Specify the path(s) to your images\n","\n","# Load a frozen Voltron (V-Cond) model & configure a vector extractor\n","vcond, preprocess = voltron.load(\"v-cond\", device=\"cuda\", freeze=True)\n","vector_extractor = voltron.instantiate_extractor(vcond)().to(\"cuda\")\n","m,v,r = reshape_vcond(path, vcond, preprocess, vector_extractor)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"afcMWY6uoiB8","executionInfo":{"status":"ok","timestamp":1715477069387,"user_tz":420,"elapsed":2438,"user":{"displayName":"Roy Y","userId":"06449485779154380910"}},"outputId":"b7257f7e-6506-4e19-8ef4-1160e7138849"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([224, 224])\n","torch.Size([224, 224])\n","torch.Size([224, 224])\n"]}]},{"cell_type":"markdown","source":["Below is code for V-dual and V-gen."],"metadata":{"id":"OeuCYV5rOu5k"}},{"cell_type":"code","source":["from torchvision.transforms import ToTensor\n","from PIL import Image\n","import torch\n","import torch.nn.functional as F\n","from torchvision.io import read_image\n","import voltron\n","\n","# V-dual and V-gen takes in a dual frame (+optional lang). Tensor must be dim5 and stacked by pairs (shape[5,2, ., ., .])\n","# Looks at image PAIRS, so when uploading images make sure there is an even number\n","\n","def process_img(path):\n","  image_tensor = read_image(path)\n","  if image_tensor.shape[0] != 3:\n","      image_tensor = image_tensor[:3, :, :]\n","  img = preprocess(image_tensor)[None, ...].to(\"cuda\")\n","  return img\n","\n","def create_image_pairs(image_paths):\n","  batch = []\n","\n","  # Process and pair images\n","  for i in range(0, len(image_paths), 2):\n","      img1 = process_img(image_paths[i])\n","      img2 = process_img(image_paths[i + 1])\n","      pair = torch.cat([img1, img2], dim=0)  # Stack images (dual pairs)\n","      batch.append(pair)\n","\n","  # Stack all pairs to form the final batch tensor (should be 5 dimensions)\n","  batch_tensor = torch.stack(batch, dim=0).to(\"cuda\")\n","  return batch_tensor\n","\n","def reshape_v(m,v,r):\n","  multimodal = m.unsqueeze(-1) # Add extra dim for interpolate\n","  visual = v.unsqueeze(-1)\n","  vec_rep = r.unsqueeze(-1)\n","  H,W = 224,224 # Behavior images were shape [4,784,784] when i was testing, but preprocess makes them 224x224. changing H,W to 224 or 784 both work\n","\n","  multimodal_resized = F.interpolate(multimodal, size=(H,W), mode='bilinear', align_corners=False)\n","  visual_resized = F.interpolate(visual, size=(H,W), mode='bilinear', align_corners=False)\n","  vec_rep = vec_rep.unsqueeze(-1)\n","  vec_rep_resized = F.interpolate(vec_rep, size=(H,W), mode='bilinear', align_corners=False)\n","\n","  multimodel_og_dim = multimodal_resized[0, 0, :, :]\n","  visual_og_dim = visual_resized[0, 0, :, :]\n","  vec_rep_og_dim = vec_rep_resized[0, 0, :, :]\n","\n","  return multimodel_og_dim, visual_og_dim, vec_rep_og_dim\n","\n","def get_reshaped_tensors(list_of_images, model_name):\n","  batch_tensor = create_image_pairs(list_of_images) #already set device to cuda\n","  model, preprocess = voltron.load(model_name, device=\"cuda\", freeze=True)\n","  vector_extractor = voltron.instantiate_extractor(model)().to(\"cuda\")\n","\n","  lang = [\"\"] # Empty, can specify as needed for model\n","\n","  # Extract both multimodal AND vision-only embeddings!\n","  multimodal_embeddings = model(batch_tensor, lang, mode=\"multimodal\")\n","  visual_embeddings = model(batch_tensor, mode=\"visual\")\n","\n","  # Use the `vector_extractor` to output dense vector representations for downstream applications!\n","  #   => Pass this representation to model of your choice (object detector, control policy, etc.)\n","  representation = vector_extractor(multimodal_embeddings)\n","  m,v,r = reshape_v(multimodal_embeddings,visual_embeddings,representation) # Reshape to original H,W\n","  return m,v,r\n","\n","# TODO: need to define list_of_images. can be a folder of imgs, just make sure there is an even # of imgs\n","list_of_images = [] # Cannot be empty otherwise error\n","model_names = [\"v-dual\", \"v-gen\"] # Models that use dual frames\n","multimodal, visual, representation = get_reshaped_tensors(list_of_images, model_names[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rxzbncUpHeCd","executionInfo":{"status":"ok","timestamp":1715474369250,"user_tz":420,"elapsed":1999,"user":{"displayName":"Roy Y","userId":"06449485779154380910"}},"outputId":"30e8d935-35d8-45a9-9f5d-59516b4f6cf6"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([224, 224])\n","torch.Size([224, 224])\n","torch.Size([224, 224])\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"p9rK-OkYYgyY"}}]}